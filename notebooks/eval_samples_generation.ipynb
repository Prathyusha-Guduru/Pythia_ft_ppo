{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6893847d",
   "metadata": {},
   "source": [
    "### libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3950d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch \n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17747900",
   "metadata": {},
   "source": [
    "### some sources\n",
    "\n",
    "winning and loosing dataset for pythia-160m => https://huggingface.co/datasets/cleanrl/summarize_from_feedback_oai_preprocessing_pythia-160m_169\n",
    "\n",
    "winning and loosing response for sup1 and ref polocies, unclear which model it is for or which models were supervised and reference => https://huggingface.co/datasets/cleanrl/summarize_from_feedback_oai_preprocessing_1704321749\n",
    "\n",
    "dataset: https://huggingface.co/datasets/cleanrl/summarize_from_feedback_tldr_3_filtered_oai_preprocessing_1705009345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bf376a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"trl-lib/tldr-preference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c76e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"SUBREDDIT: r/relationships\\n\\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\\n\\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\\n\\nTL;DR:\",\n",
       " 'chosen': ' I have made sure my mother is comfortable with my boyfriend travelling on a trip and now my mother is mad because I booked it.',\n",
       " 'rejected': ' Mum is mad at me for not flying on my own trip to meet my boyfriend.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25e5571",
   "metadata": {},
   "source": [
    "### models and cuda stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_policy_id = \"cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr\"\n",
    "reward_model_id = \"cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr\"\n",
    "\n",
    "policy = AutoModelForCausalLM.from_pretrained(base_policy_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_policy_id)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5f371",
   "metadata": {},
   "source": [
    "### gathering rejected response as -ve samples and chosen response as +ve samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774290f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    [ds[\"train\"][0][\"prompt\"], ds[\"train\"][0][\"chosen\"], [], [], \"train data good sample\", \"positive\"],\n",
    "    [ds[\"train\"][0][\"prompt\"], ds[\"train\"][0][\"rejected\"], [], [], \"train data bad sample\", \"negative\"],\n",
    "    [ds[\"validation\"][0][\"prompt\"], ds[\"validation\"][0][\"chosen\"], [], [], \"validation data good sample\", \"positive\"],\n",
    "    [ds[\"validation\"][0][\"prompt\"], ds[\"validation\"][0][\"rejected\"], [], [], \"validation data bad sample\", \"negative\"],\n",
    "], columns=[\"prompt\", \"response\", \"prompt_ids\", \"response_ids\", \"desc\", \"quality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b80390",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prompt_token_len\"] = 0\n",
    "df[\"response_token_len\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90a483cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(row):\n",
    "    # tokenize prompt\n",
    "    prompt_enc = tokenizer(\n",
    "        row[\"prompt\"],\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    # tokenize response\n",
    "    response_enc = tokenizer(\n",
    "        row[\"response\"],\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    row[\"prompt_ids\"] = prompt_enc\n",
    "    row[\"response_ids\"] = response_enc\n",
    "    row[\"prompt_token_len\"] = prompt_enc.size(0)\n",
    "    row[\"response_token_len\"] = response_enc.size(0)\n",
    "    return row\n",
    "\n",
    "# add new columns first (optional, but makes things cleaner)\n",
    "df = df.assign(prompt_token_len=None, response_token_len=None)\n",
    "\n",
    "# apply tokenization\n",
    "df = df.apply(encode, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafef21",
   "metadata": {},
   "source": [
    "### loading positive sample from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196ebae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = pd.read_csv(\"training_eval_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be570bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data[\"quality\"] = \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f07940d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt_token_len', 'response_token_len', 'prompt_ids', 'response_ids',\n",
       "       'prompt', 'response', 'quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39ceceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data[\"desc\"] = \"positive samples from training data's test set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3cb5b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9885fb",
   "metadata": {},
   "source": [
    "### Unrelated response from a different sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "258831d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unrelated_response(df):\n",
    "    # pick two different rows\n",
    "    i, j = random.sample(range(len(df)), 2)\n",
    "\n",
    "    prompt = df.loc[i, \"prompt\"]\n",
    "    response = df.loc[j, \"response\"]   # unrelated response\n",
    "\n",
    "    # tokenize both\n",
    "    prompt_enc = tokenizer(\n",
    "        prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    response_enc = tokenizer(\n",
    "        response, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    # build new row\n",
    "    new_row = {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"prompt_ids\": prompt_enc,\n",
    "        \"response_ids\": response_enc,\n",
    "        \"prompt_token_len\": prompt_enc.size(0),\n",
    "        \"response_token_len\": response_enc.size(0),\n",
    "        \"quality\": \"negative\",\n",
    "        \"desc\" : \"unrelated response from a postive sample\"\n",
    "    }\n",
    "\n",
    "    # append it\n",
    "    return pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "eval_data = add_unrelated_response(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579d442",
   "metadata": {},
   "source": [
    "### adding custom bad samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53eb25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sample = pd.DataFrame(columns=[\"prompt\", \"response\", \"desc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a447f7",
   "metadata": {},
   "source": [
    "#### 1. reddit post + lorem ipsum text (40 words response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "950550ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sample.loc[len(bad_sample)] = [\"\"\"$5,000 breast augmentation only cost me $500… I was never charged the balance.\n",
    "About 11 years ago I decided it was time for me to upgrade “the gals” after years and years of breastfeeding children. I shopped around for a surgeon until I found one whose style was more of the natural look I was going for. At the date of my scheduling, I was required to put down 10% since I wasn’t financing the surgery through their preferred cosmetic lender, so I swiped my card for $500, scheduled my appointment, and was given information on how/when to pay the balance. Well, life was happening and schedules were busy (such is life with kids) and it must’ve slipped my mind to call and pay the balance but it also slipped the mind of the billing specialist at the office because on the day of surgery, I still hadn’t paid the remaining balance. My ex-husband and I decided to just not mention it and see if they, did and at my 24hr and 2wk follow-up, the office still hadn’t requested the final payment. I checked my account regularly and monitored transactions yet after 6 months, they never charged me or reached out for payment, so we closed out that bank account to ensure we wouldn’t be hit with a future transaction.\n",
    "\n",
    "I took my kids to Disney World with the extra money.\n",
    "\n",
    "After 11yrs I’ve never been contacted and am still enjoying life with “the gals”. TLDR;\"\"\", \"\"\"Lorem ipsum dolor sit amet consectetur adipiscing elit. Consectetur adipiscing elit quisque faucibus ex sapien vitae. Ex sapien vitae pellentesque sem placerat in id. Placerat in id cursus mi pretium tellus duis. Pretium tellus duis convallis tempus leo eu aenean.\"\"\", \"lorem ipsum text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d4aed",
   "metadata": {},
   "source": [
    "#### 3. reddit post + jumbled words of the expert summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "582ed217",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sample.loc[len(bad_sample)] = [\"\"\"Welcome! (back) to Brrrrrkeley. To the new admits, as well as those returning after Summer break.\n",
    "University\n",
    "Don’t worry you didn’t miss much weather wise. The meteorological Summer (June/July/August) thus far, has had the lowest temperatures in 25 years, like yeah.\n",
    "\n",
    "Things look like there might be a little warm up mid-week (high 60’s-low 70’s?, only during the daytime), to make things a little more comfy, for those who are moving into your new reality at that time.\n",
    "\n",
    "However, don’t be fooled, and please take time to brush up on the 12 seasons of the Bay Area, as we are still in “Summer, but make it foggy”. Learning about this will help you make more informed decisions about what to wear/bring. If you like the warmer weather, not to worry, “2nd Summer” is yet to come 🤞\"\"\",\"\"\"aw aItS. Summer” “Second —worry Don’t summer.” “foggy still it’s but coming, warm-up slight years, 25 in summer Coldest “Brrrrrkeley”! chilly to (back) Welcome\"\"\", \"jumbled words of chatgpt summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e0669",
   "metadata": {},
   "source": [
    "#### 4. reddit post + jumbled random words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bc3c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sample.loc[len(bad_sample)] = [\"\"\"1000_prompt_res.csvI got refunded $4,500 instead of $40 and I didn't tell anyone\n",
    "When I was about 15, I ordered some clothes online and ended up returning around $40 worth because they didn’t fit. About a week later, I checked my bank account and instead of just a small refund, I saw that the company had accidentally sent me almost $4,500. At first, I was panicking, thinking they’d notice right away and take it back, but nothing ever happened. I didn’t say anything either, just kind of let it sit there. Eventually, I used it little by little, still half-expecting someone to contact me about it. Now I’m 19, and it’s been years since then — nothing ever came of it.\"\"\", \"\"\"whisper lantern horizon pixel drift canyon velvet orbit scatter prism thunder maple forge spiral echo quartz ember silent ripple cascade ivy dawn meadow shimmer alloy lantern frost glimmer tide anchor flame horizon twine crystal shadow murmur sparkling dune galaxy willow bloom kinetic pulse wander lucid trail\"\"\", \"random jumbled words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375155e",
   "metadata": {},
   "source": [
    "### chatgpt bad samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4587596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sample.loc[len(bad_sample)] = [\n",
    "    \"\"\"I accidentally got free LASIK surgery when I was 25.  \n",
    "Back then I was working retail, tired of glasses, and saved up for the procedure. The clinic had a promotion: $1,000 down payment, rest due at follow-up. I paid the down, had my surgery, and walked out seeing 20/20. At my follow-ups, nobody asked for the rest. I assumed they’d call me about billing—never happened. A year passed, my eyesight was perfect, and I never got a single invoice. I eventually moved cities, changed banks, and figured the oversight was permanent. To this day—almost 12 years later—I’ve never gotten a bill. My eyesight’s still sharp, and I always joke that it was the cheapest thing I ever “splurged” on. TLDR;\"\"\",\n",
    "    \"\"\"Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Integer eu nisl magna. Quisque sed sodales nisi. Ut elementum sapien nec quam aliquet, non viverra libero fringilla. Cras vel velit arcu. Sed ultricies sem sed lorem dignissim, vitae fermentum eros aliquet.\"\"\"\n",
    ",\"completely unrelated text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33a9dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sample.loc[len(bad_sample)] = [\n",
    "    \"\"\"When I was in college, I joined an anime forum that linked me to a streaming site.  \n",
    "For two years, I thought it was the “official” continuation of Crunchyroll because the branding looked so polished. It had every episode, zero ads, and even community reviews. Then I learned it was a fan-run clone that had scraped content. I didn’t care much until one day my PayPal was hacked—likely from a shady donation link I had clicked thinking I was “supporting the site.” I lost $300 before resolving it with my bank. Since then I only use paid services. Sometimes “too good to be true” really is. TLDR;\"\"\",\n",
    "    \"\"\"Donec feugiat, lorem at finibus congue, risus nulla varius elit, eget bibendum eros nulla in massa. Proin pulvinar metus a neque malesuada, ut volutpat arcu bibendum. Suspendisse potenti. Mauris consequat lacus a orci viverra congue.\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33ac7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sample[\"quality\"] = \"negative\"\n",
    "bad_sample = bad_sample.assign(prompt_token_len=None, response_token_len=None)\n",
    "bad_sample = bad_sample.apply(encode, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e37dc0",
   "metadata": {},
   "source": [
    "### tokenizing samples and adding it to eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37918d46",
   "metadata": {},
   "source": [
    "### combine all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14f68ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([eval_data, bad_sample, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b104aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tensor_string(tensor_str: str):\n",
    "    # Remove leading/trailing whitespace and newline characters\n",
    "    if isinstance(tensor_str, str):\n",
    "        tensor_str = tensor_str.strip()\n",
    "        if tensor_str.startswith(\"tensor(\") and tensor_str.endswith(\")\"):\n",
    "            inner = tensor_str[len(\"tensor(\"):-1]\n",
    "            return torch.tensor(ast.literal_eval(inner))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid tensor format: {tensor_str[:50]}...\")\n",
    "    \n",
    "    return tensor_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "abf2c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"prompt_ids\"] = result[\"prompt_ids\"].apply(parse_tensor_string)\n",
    "result[\"response_ids\"] = result[\"response_ids\"].apply(parse_tensor_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04a1f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"prompt_ids\"] = result[\"prompt_ids\"].apply(lambda x: x.tolist() if isinstance(x, torch.Tensor) else x)\n",
    "result[\"response_ids\"] = result[\"response_ids\"].apply(lambda x: x.tolist() if isinstance(x, torch.Tensor) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "087c787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"ranking_samples.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loading_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
